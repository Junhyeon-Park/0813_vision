{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 삼성 DS-KAIST AI Expert 프로그램\n",
    "- 강의: 황성주 교수님\n",
    "- 조교: 허자욱 (jayheo@kaist.ac.kr), 박준현 (pjh2941@kaist.ac.kr)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet V2 Keypoint Detection\n",
    "\n",
    "본 실습에서는 Tensorflow(v1) 프레임워크를 이용하여, keypoint detection 모델을 mobilenet v2로 구조로 구현하여 실시간으로 키포인트를 탐지할 수 있는 모델을 구현하는 내용을 다루고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# disable numpy version warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "# custom functions\n",
    "from dataset import Dataset\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments for training model\n",
    "parser = argparse.ArgumentParser(description='Hyperparameter for training a Mobilenet V2')\n",
    "parser.add_argument('--input_width',      help='Rescale the image in x-axis.', type=int, default=192)\n",
    "parser.add_argument('--input_height',     help='Rescale the image in y-axis.', type=int, default=192)\n",
    "parser.add_argument('--batchsize',        help='Size of the batches.',         type=int, default=16)\n",
    "parser.add_argument('--gpu',              help='Id of the GPU to use (as reported by nvidia-smi).', type=str, default=\"1\")\n",
    "parser.add_argument('--epoch',            help='Number of epochs to train.',   type=int, default=30)\n",
    "parser.add_argument('--save_feq',         help='Frequency of saving checkpoints in epochs.', type=int, default=10)\n",
    "parser.add_argument('--checkpoint_path',  help='Path to sotre snapshots of models during training.', type=str, default='./checkpoints')\n",
    "parser.add_argument('--lr',               help='Learning rate.',                   type=float, default=1e-3)\n",
    "parser.add_argument('--lr_decay_rate',    help='Decay rate of the learning rate.', type=float, default=0.95)\n",
    "parser.add_argument('--random_transform', help='Able/Disable data augmentation.',  type=bool, default=False)\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Image Data\n",
    "\n",
    "3가지 과정으로 전처리 과정이 이루어져 있습니다.\n",
    "- `pad_and_reshape`: 인풋을 일정한 크기로 맞춰주는 과정입니다.\n",
    "- `render_gaussian_heatmap`: 좌표 정보를 사용하여 학습에 사용될 target heatmap을 생성하는 과정입니다.\n",
    "- `generate_batch`: 좌표와 이미지를 Batch로 묶어주는 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![0001](image/0001.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_reshape(img, coord, target_shape=(args.input_width, args.input_height)):\n",
    "    # compute how much padding need.\n",
    "    h,w,_ = img.shape\n",
    "    max_dim = max(h, w)\n",
    "    delta_w = max_dim - w\n",
    "    delta_h = max_dim - h\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    # pad the image to match ratio of target shape\n",
    "    resized_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0,0,0])\n",
    "    draw = resized_img.copy()\n",
    "    scale = max_dim / target_shape[0]\n",
    "\n",
    "    # resize the image with target shape\n",
    "    resized_img = cv2.resize(resized_img, target_shape)\n",
    "    \n",
    "    # pad the coordinate to match ratio of target shape\n",
    "    for i in range(coord.shape[0]):\n",
    "        coord[i, 0] += left \n",
    "        coord[i, 1] += top\n",
    "    \n",
    "    # resize the coordinate with the computed scale\n",
    "    resized_coord = coord.copy()\n",
    "    for i in range(resized_coord.shape[0]):\n",
    "        resized_coord[i, 0] = int(resized_coord[i, 0] / scale)\n",
    "        resized_coord[i, 1] = int(resized_coord[i, 1] / scale)\n",
    "    \n",
    "    return resized_img, resized_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![0002](image/0002.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_gaussian_heatmap(coord):\n",
    "    sigmas = 3\n",
    "    \n",
    "    input_shape  = [args.input_height, args.input_width]\n",
    "    output_shape = [args.input_height//2, args.input_width//2]\n",
    "    num_kps = coord.shape[1]\n",
    "    \n",
    "    x = [i for i in range(output_shape[1])]\n",
    "    y = [i for i in range(output_shape[0])]\n",
    "    xx, yy = tf.meshgrid(x, y)\n",
    "    xx = tf.reshape(tf.to_float(xx), (1, output_shape[0], output_shape[1], 1))\n",
    "    yy = tf.reshape(tf.to_float(yy), (1, output_shape[0], output_shape[1], 1))\n",
    "    \n",
    "    x = tf.floor(tf.reshape(coord[:, :, 0], [-1, 1, 1, num_kps]) / input_shape[1] * output_shape[1] + 0.5)\n",
    "    y = tf.floor(tf.reshape(coord[:, :, 1], [-1, 1, 1, num_kps]) / input_shape[0] * output_shape[0] + 0.5)\n",
    "    \n",
    "    heatmap = tf.exp(-(((xx-x)/tf.to_float(sigmas))**2)/tf.to_float(2)\n",
    "                     -(((yy-y)/tf.to_float(sigmas))**2)/tf.to_float(2))\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![0003](image/0003.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(train_data, train_indices, step, batchsize=args.batchsize):\n",
    "    image_lst = []\n",
    "    coord_lst = []\n",
    "    flags_lst = []\n",
    "    for j in train_indices[step * batchsize:(step+1) * batchsize]:\n",
    "        im2read = cv2.imread(train_data[j]['image_path'])\n",
    "        joints = np.array(train_data[j]['joints']).reshape(14, 3)\n",
    "        keypoints = joints[:, :2].astype(np.float32)\n",
    "        flags = joints[:, 2].astype(np.float32)\n",
    "        reshaped_image, reshaped_keypoint = pad_and_reshape(im2read, keypoints)            \n",
    "                \n",
    "        image_lst.append(reshaped_image)\n",
    "        coord_lst.append(reshaped_keypoint)\n",
    "        flags_lst.append(flags)\n",
    "                \n",
    "    np_image = np.array(image_lst, dtype=np.float32)\n",
    "    np_coord = np.array(coord_lst, dtype=np.float32)\n",
    "    np_flags = np.array(flags_lst, dtype=np.float32)\n",
    "            \n",
    "    feed_dict = dict()\n",
    "    feed_dict[input_image] = np_image\n",
    "    feed_dict[input_coord] = np_coord\n",
    "    feed_dict[input_flags] = np_flags\n",
    "    \n",
    "    return feed_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Dataset\n",
    "\n",
    "학습 및 테스트에 사용할 데이터셋을 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset()\n",
    "train_data = d.load_frame_data(task='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Function of Mobilenet V2\n",
    "- `is_trainable`: 학습/테스트 모드를 설정합니다.\n",
    "- `max_pool`: convolution layer의 feature-map의 크기를 줄이고, 위치나 이동에 좀 더 강인한 특징을 추출하기 위한 과정입니다.\n",
    "- `upsample`: feature들을 하나의 output으로 묶어주기 위하여 dimension을 맞춰주는 작업입니다.\n",
    "- `convb`: stem layer에 사용될 기본적인 convolution layer입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "_init_norm = tf.truncated_normal_initializer(stddev=0.01)\n",
    "_init_zero = slim.init_ops.zeros_initializer()\n",
    "_l2_regularizer_00004 = tf.contrib.layers.l2_regularizer(0.00004)\n",
    "_trainable = True\n",
    "\n",
    "\n",
    "def is_trainable(trainable=True):\n",
    "    global _trainable\n",
    "    _trainable = trainable\n",
    "    \n",
    "\n",
    "def max_pool(inputs, k_h, k_w, s_h, s_w, name, padding=\"SAME\"):\n",
    "    return tf.nn.max_pool(inputs,\n",
    "                          ksize=[1, k_h, k_w, 1],\n",
    "                          strides=[1, s_h, s_w, 1],\n",
    "                          padding=padding,\n",
    "                          name=name)\n",
    "\n",
    "\n",
    "def upsample(inputs, factor, name):\n",
    "    return tf.image.resize_bilinear(inputs, [int(inputs.get_shape()[1]) * factor, int(inputs.get_shape()[2]) * factor],\n",
    "                                    name=name)\n",
    "\n",
    "\n",
    "def convb(input, k_h, k_w, c_o, stride, name, relu=True):\n",
    "    with slim.arg_scope([slim.batch_norm], decay=0.999, fused=True, is_training=_trainable):\n",
    "        output = slim.convolution2d(\n",
    "            inputs=input,\n",
    "            num_outputs=c_o,\n",
    "            kernel_size=[k_h, k_w],\n",
    "            stride=stride,\n",
    "            normalizer_fn=slim.batch_norm,\n",
    "            weights_regularizer=_l2_regularizer_00004,\n",
    "            weights_initializer=_init_xavier,\n",
    "            biases_initializer=_init_zero,\n",
    "            activation_fn=tf.nn.relu if relu else None,\n",
    "            scope=name,\n",
    "            trainable=_trainable)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Function of Mobilenet V2\n",
    "- `separable_conv`: 기본적인 convolution 연산을 두 단계로 나누어 계산량을 줄이기 위한 과정입니다.\n",
    "- `inverted_bottleneck`: 기존의 residual_bottleneck과정을 정 반대로 수행하여 GPU Computation에 필요한 메모리를 줄이기 위한 과정입니다.\n",
    "![0004](image/0004.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separable_conv(input, c_o, k_s, stride, scope):\n",
    "    with slim.arg_scope([slim.batch_norm],\n",
    "                        decay=0.999,\n",
    "                        fused=True,\n",
    "                        is_training=_trainable,\n",
    "                        activation_fn=tf.nn.relu6):\n",
    "        output = slim.separable_convolution2d(input,\n",
    "                                              num_outputs=None,\n",
    "                                              stride=stride,\n",
    "                                              trainable=_trainable,\n",
    "                                              depth_multiplier=1.0,\n",
    "                                              kernel_size=[k_s, k_s],\n",
    "                                              weights_initializer=_init_xavier,\n",
    "                                              weights_regularizer=_l2_regularizer_00004,\n",
    "                                              biases_initializer=None,\n",
    "                                              scope=scope + '_depthwise')\n",
    "\n",
    "        output = slim.convolution2d(output,\n",
    "                                    c_o,\n",
    "                                    stride=1,\n",
    "                                    kernel_size=[1, 1],\n",
    "                                    weights_initializer=_init_xavier,\n",
    "                                    biases_initializer=_init_zero,\n",
    "                                    normalizer_fn=slim.batch_norm,\n",
    "                                    trainable=_trainable,\n",
    "                                    weights_regularizer=None,\n",
    "                                    scope=scope + '_pointwise')\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def inverted_bottleneck(inputs, up_channel_rate, channels, subsample, k_s=3, scope=\"\"):\n",
    "    with tf.variable_scope(\"inverted_bottleneck_%s\" % scope):\n",
    "        with slim.arg_scope([slim.batch_norm],\n",
    "                            decay=0.999,\n",
    "                            fused=True,\n",
    "                            is_training=_trainable,\n",
    "                            activation_fn=tf.nn.relu6):\n",
    "            stride = 2 if subsample else 1\n",
    "\n",
    "            output = slim.convolution2d(inputs,\n",
    "                                        up_channel_rate * inputs.get_shape().as_list()[-1],\n",
    "                                        stride=1,\n",
    "                                        kernel_size=[1, 1],\n",
    "                                        weights_initializer=_init_xavier,\n",
    "                                        biases_initializer=_init_zero,\n",
    "                                        normalizer_fn=slim.batch_norm,\n",
    "                                        weights_regularizer=None,\n",
    "                                        scope=scope + '_up_pointwise',\n",
    "                                        trainable=_trainable)\n",
    "\n",
    "            output = slim.separable_convolution2d(output,\n",
    "                                                  num_outputs=None,\n",
    "                                                  stride=stride,\n",
    "                                                  depth_multiplier=1.0,\n",
    "                                                  kernel_size=k_s,\n",
    "                                                  weights_initializer=_init_xavier,\n",
    "                                                  weights_regularizer=_l2_regularizer_00004,\n",
    "                                                  biases_initializer=None,\n",
    "                                                  padding=\"SAME\",\n",
    "                                                  scope=scope + '_depthwise',\n",
    "                                                  trainable=_trainable)\n",
    "\n",
    "            output = slim.convolution2d(output,\n",
    "                                        channels,\n",
    "                                        stride=1,\n",
    "                                        kernel_size=[1, 1],\n",
    "                                        activation_fn=None,\n",
    "                                        weights_initializer=_init_xavier,\n",
    "                                        biases_initializer=_init_zero,\n",
    "                                        normalizer_fn=slim.batch_norm,\n",
    "                                        weights_regularizer=None,\n",
    "                                        scope=scope + '_pointwise',\n",
    "                                        trainable=_trainable)\n",
    "            if inputs.get_shape().as_list()[-1] == channels:\n",
    "                output = tf.add(inputs, output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task] MobileNet V2 Network\n",
    "\n",
    "아래에 주어진 표를 참고하여 빈칸을 채워 mobilenet v2를 구현해보세요.\n",
    "- `(up_channel_rate, channels, subsample, k_s)` : 해당 형태로 숫자를 채워넣으면 됩니다. (상단의 `inverted_bottleneck` 함수 참고)\n",
    "- 현재 밑에 작성된 학습 과정이 주석처리 되어, 다음 단계인 pretrained checkpoint를 이용한 테스트 과정이 호출됩니다. Task1이 올바르게 작성되었을 경우 테스트는 문제없이 진행됩니다.\n",
    "- 테스트 과정이 잘 되었을 경우 학습 과정의 주석을 제거하여 학습이 어떻게 진행되는지 확인 가능합니다. (30 epochs)\n",
    "![0005](image/0005.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_KPOINTS = 14\n",
    "STAGE_NUM = 6\n",
    "out_channel_ratio = lambda d: max(int(d * 0.75), 8)\n",
    "up_channel_ratio = lambda d: int(d * 1.)\n",
    "out_channel_cpm = lambda d: max(int(d * 0.75), 8)\n",
    "\n",
    "\n",
    "def cpm_mobilenet_v2(input, trainable):\n",
    "    is_trainable(trainable)\n",
    "    \n",
    "    # Input : 224 x 224 x 3\n",
    "    net = convb(input, 3, 3, 24, 2, name=\"Conv2d_0\")\n",
    "\n",
    "    with tf.variable_scope('MobilenetV2'):\n",
    "        # Input : 112 x 112 x 3\n",
    "        mv2_branch_0 = slim.stack(net, inverted_bottleneck,\n",
    "                                  [\n",
    "                                      ...,\n",
    "                                  ], scope=\"MobilenetV2_part_0\")\n",
    "\n",
    "        # Input : 56 x 56 x 3\n",
    "        mv2_branch_1 = slim.stack(mv2_branch_0, inverted_bottleneck,\n",
    "                                  [\n",
    "                                      ...,\n",
    "                                  ], scope=\"MobilenetV2_part_1\")\n",
    "\n",
    "        # Input : 28 x 28 x 3\n",
    "        mv2_branch_2 = slim.stack(mv2_branch_1, inverted_bottleneck,\n",
    "                                  [\n",
    "                                      ...,\n",
    "                                  ], scope=\"MobilenetV2_part_2\")\n",
    "\n",
    "        # Input : 14 x 14 x 3\n",
    "        mv2_branch_3 = slim.stack(mv2_branch_2, inverted_bottleneck,\n",
    "                                  [\n",
    "                                      ...,\n",
    "                                  ], scope=\"MobilenetV2_part_3\")\n",
    "\n",
    "        # Input : 7 x 7 x 3\n",
    "        mv2_branch_4 = slim.stack(mv2_branch_3, inverted_bottleneck,\n",
    "                                  [\n",
    "                                      ...,\n",
    "                                  ], scope=\"MobilenetV2_part_4\")\n",
    "\n",
    "        # concat for CPM\n",
    "        cancat_mv2 = tf.concat(\n",
    "            [\n",
    "                max_pool(mv2_branch_0, 4, 4, 4, 4, name=\"mv2_0_max_pool\"),\n",
    "                max_pool(mv2_branch_1, 2, 2, 2, 2, name=\"mv2_1_max_pool\"),\n",
    "                mv2_branch_2,\n",
    "                upsample(mv2_branch_3, 2, name=\"mv2_3_upsample\"),\n",
    "                upsample(mv2_branch_4, 4, name=\"mv2_4_upsample\")\n",
    "            ]\n",
    "            , axis=3)\n",
    "\n",
    "    with tf.variable_scope(\"Convolutional_Pose_Machine\"):\n",
    "        l2s = []\n",
    "        prev = None\n",
    "        for stage_number in range(STAGE_NUM):\n",
    "            if prev is not None:\n",
    "                inputs = tf.concat([cancat_mv2, prev], axis=3)\n",
    "            else:\n",
    "                inputs = cancat_mv2\n",
    "\n",
    "            kernel_size = 7\n",
    "            lastest_channel_size = 128\n",
    "            if stage_number == 0:\n",
    "                kernel_size = 3\n",
    "                lastest_channel_size = 512\n",
    "\n",
    "            _ = slim.stack(inputs, inverted_bottleneck,\n",
    "                           [\n",
    "                               (2, 24, 0, kernel_size),\n",
    "                               (4, 24, 0, kernel_size),\n",
    "                               (4, 24, 0, kernel_size),\n",
    "                           ], scope=\"stage_%d_mv2\" % stage_number)\n",
    "\n",
    "            _ = slim.stack(_, separable_conv,\n",
    "                           [\n",
    "                               (out_channel_ratio(lastest_channel_size), 1, 1),\n",
    "                               (N_KPOINTS, 1, 1)\n",
    "                           ], scope=\"stage_%d_mv1\" % stage_number)\n",
    "\n",
    "            prev = _\n",
    "            cpm_out = upsample(_, 4, \"stage_%d_out\" % stage_number)\n",
    "            l2s.append(cpm_out)\n",
    "\n",
    "    return cpm_out, l2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder, Output, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "input_image = tf.placeholder(tf.float32, [None, args.input_height, args.input_width, 3])    # [batchsize, H, W, C]\n",
    "input_coord = tf.placeholder(tf.float32, [None, 14, 2])         # [batchsize, num_kps, 2]\n",
    "input_flags = tf.placeholder(tf.float32, [None, 14])            # [batchsize, num_kps]\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(float(args.lr), global_step,\n",
    "        decay_steps=10000, decay_rate=float(args.lr_decay_rate), staircase=True)\n",
    "\n",
    "# Output\n",
    "input_heatmaps = render_gaussian_heatmap(input_coord)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "    _, pred_heatmaps_all = cpm_mobilenet_v2(input_image, True)\n",
    "    \n",
    "# Loss\n",
    "losses = []\n",
    "for idx, pred_heat in enumerate(pred_heatmaps_all):\n",
    "    reshaped_flags = tf.reshape(input_flags, [-1, 1, 1, 14])\n",
    "    loss_l2 = tf.nn.l2_loss((tf.concat(pred_heat, axis=0) - input_heatmaps) * reshaped_flags, name='loss_heatmap_stage%d' % idx)\n",
    "    losses.append(loss_l2)\n",
    "\n",
    "total_loss = tf.reduce_sum(losses) / args.batchsize\n",
    "total_loss_ll_heat = tf.reduce_sum(loss_l2) / args.batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.train.AdamOptimizer(learning_rate, epsilon = 1e-8)\n",
    "grads = optim.compute_gradients(total_loss)\n",
    "apply_gradients_op = optim.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "variable_to_average = (tf.trainable_variables() + tf.moving_average_variables())\n",
    "variables_averages_op = variable_averages.apply(variable_to_average)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.group(apply_gradients_op, variables_averages_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create session before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = get_session()\n",
    "\n",
    "# initialize variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# prepare to train\n",
    "train_indices = np.arange(len(train_data))\n",
    "num_steps = len(train_indices) // args.batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "- 다음 단계인 테스트가 되는지 먼저 확인하기 위해 주석처리 되었습니다.\n",
    "- 테스트가 정상적으로 될 경우, 해당 주석을 제거하여 학습을 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for epoch in range(args.epoch):\n",
    "    # shuffle all elements per epoch\n",
    "    shuffled_indices = train_indices.copy()\n",
    "    get_rng().shuffle(shuffled_indices)\n",
    "        \n",
    "    for i in range(num_steps):\n",
    "        feed_dict = generate_batch(train_data, shuffled_indices, i)\n",
    "        start = time.time()\n",
    "        _, loss, loss_lastlayer_heat = sess.run([train_op, total_loss, total_loss_ll_heat], \n",
    "                                            feed_dict = feed_dict)\n",
    "        duration = time.time() - start\n",
    "    \n",
    "    # print train info per epoch\n",
    "    print('epoch: %d, loss = %.2f, last_heat_loss = %.2f, duration = %.4f' % (epoch, loss, loss_lastlayer_heat, duration))      \n",
    "\n",
    "    # visualize ground truth heatmap and output heatmap\n",
    "    background = feed_dict[input_image][0]\n",
    "    flags_info = feed_dict[input_flags][0]\n",
    "    heatmaps, outputs = sess.run([input_heatmaps, pred_heat], feed_dict = feed_dict)  \n",
    "    visualize_gt_and_output(background, heatmaps[0], outputs[0], flags_info)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore pretrained model\n",
    "saver = tf.train.Saver()\n",
    "pretrain_dir = './pretrained_ckpts/model-450000'\n",
    "saver.restore(sess, pretrain_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Test with Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set trainable=False and get output heatmap\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_heatmaps, test_heatmaps_all = cpm_mobilenet_v2(input_image, False)\n",
    "\n",
    "num_steps = len(train_indices) // 1\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i in range(num_steps):\n",
    "        feed_dict = generate_batch(train_data, train_indices, i, batchsize=1)    \n",
    "        background = feed_dict[input_image][0]\n",
    "        flags_info = feed_dict[input_flags][0]\n",
    "        \n",
    "        start = time.time()\n",
    "        heatmaps, outputs = sess.run([input_heatmaps, test_heatmaps], feed_dict = feed_dict)\n",
    "        duration = time.time() - start\n",
    "        print('processing time = %.4f' % duration)\n",
    "        \n",
    "        visualize_gt_and_output(background, heatmaps[0], outputs[0], flags_info, delay=10, draw_keypoint=True)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
